{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SushmithaUW/FNC-1-Stance-Detection/blob/master/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtO9NtyfwMmv",
        "colab_type": "code",
        "outputId": "c5f7e3ef-89af-4560-9a06-40eb25a1c38d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!unzip fnc-1.zip\n",
        "!unzip utils.zip\n",
        "!unzip splits.zip"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  utils.zip\n",
            "  inflating: utils/dataset.py        \n",
            "  inflating: utils/generate_split.py  \n",
            "  inflating: utils/generate_test_splits.py  \n",
            "  inflating: utils/score.py          \n",
            " extracting: utils/__init__.py       \n",
            "Archive:  splits.zip\n",
            "  inflating: splits/dev_ids.txt      \n",
            "  inflating: splits/test_ids.txt     \n",
            "  inflating: splits/training_ids.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1xQ0sj8wR0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from utils.dataset import DataSet\n",
        "from utils.generate_test_splits import split\n",
        "from os import path\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "import pylab as py\n",
        "from scipy.sparse import hstack\n",
        "from scipy.sparse import coo_matrix\n",
        "from tqdm import tqdm\n",
        "from scipy import sparse\n",
        "import csv\n",
        "import random\n",
        "import numpy\n",
        "import score\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "import scipy\n",
        "import gensim\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import tree\n",
        "#from langdetect import detect\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from csv import DictReader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voQdW_7OvXqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataSet():\n",
        "    def __init__(self, name=\"train\", path=\"fnc-1\"):\n",
        "        self.path = path\n",
        "\n",
        "        print(\"Reading dataset\")\n",
        "        bodies = name+\"_bodies.csv\"\n",
        "        stances = name+\"_stances.csv\"\n",
        "\n",
        "        self.stances = self.read(stances)\n",
        "        articles = self.read(bodies)\n",
        "        self.articles = dict()\n",
        "\n",
        "        #make the body ID an integer value\n",
        "        for s in self.stances:\n",
        "            s['Body ID'] = int(s['Body ID'])\n",
        "\n",
        "        #copy all bodies into a dictionary\n",
        "        for article in articles:\n",
        "            self.articles[int(article['Body ID'])] = article['articleBody']\n",
        "\n",
        "        print(\"Total stances: \" + str(len(self.stances)))\n",
        "        print(\"Total bodies: \" + str(len(self.articles)))\n",
        "\n",
        "\n",
        "\n",
        "    def read(self,filename):\n",
        "        print (self.path + \"/\" + filename)\n",
        "        rows = []\n",
        "        with open(self.path + \"/\" + filename, \"r\", encoding='utf-8') as table:\n",
        "            r = DictReader(table)\n",
        "\n",
        "            for line in r:\n",
        "                rows.append(line)\n",
        "        return rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHlsw6uSwXPC",
        "colab_type": "code",
        "outputId": "d6984466-6dce-4fc7-f57d-83f674a1e5c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "dataset = DataSet()\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "c = DataSet(\"competition_test\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading dataset\n",
            "fnc-1/train_stances.csv\n",
            "fnc-1/train_bodies.csv\n",
            "Total stances: 49972\n",
            "Total bodies: 1683\n",
            "Reading dataset\n",
            "fnc-1/competition_test_stances.csv\n",
            "fnc-1/competition_test_bodies.csv\n",
            "Total stances: 25413\n",
            "Total bodies: 904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPMP7l2swZOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the bodies of training data points\n",
        "def get_bodies(data):\n",
        "\tbodies = []\n",
        "\tfor i in range(len(data)):\n",
        "\t\tbodies.append(dataset.articles[data[i]['Body ID']])\t\n",
        "\treturn bodies\n",
        "\n",
        "# Get the headlines of training data points\n",
        "def get_headlines(data):\n",
        "\theadlines = []\n",
        "\tfor i in range(len(data)):\n",
        "\t\theadlines.append(data[i]['Headline'])\n",
        "\treturn headlines\n",
        "\n",
        "# Tokenisation, Normalisation, Capitalisation, Non-alphanumeric removal, Stemming-Lemmatization\n",
        "def preprocess(string):\n",
        "\t# to lowercase, non-alphanumeric removal\n",
        "\tstep1 = \" \".join(re.findall(r'\\w+', string, flags=re.UNICODE)).lower()\n",
        "\tstep2 = [lemmatizer.lemmatize(t).lower() for t in nltk.word_tokenize(step1)]\n",
        "\n",
        "\treturn step2\n",
        "\n",
        "\n",
        "# Function for extracting word overlap\n",
        "def extract_word_overlap(headlines, bodies):\n",
        "\tword_overlap = []\n",
        "\tfor i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
        "\t\tpreprocess_headline = preprocess(headline)\n",
        "\t\tpreprocess_body = preprocess(body)\n",
        "\t\tfeatures = len(set(preprocess_headline).intersection(preprocess_body)) / float(len(set(preprocess_headline).union(preprocess_body)))\n",
        "\t\tword_overlap.append(features)\n",
        "\n",
        "\t\t# Convert the list to a sparse matrix (in order to concatenate the cos sim with other features)\n",
        "\t\tword_overlap_sparse = scipy.sparse.coo_matrix(numpy.array(word_overlap)) \n",
        "\n",
        "\treturn word_overlap_sparse\n",
        "\n",
        "# Function for extracting tf-idf vectors (for both the bodies and the headlines).\n",
        "def extract_tfidf(training_headlines, training_bodies, dev_headlines, dev_bodies, test_headlines, test_bodies):\n",
        "\t# Body vectorisation\n",
        "\tbody_vectorizer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, stop_words='english')#, max_features=1024)\n",
        "\tbodies_tfidf = body_vectorizer.fit_transform(training_bodies)\n",
        "\n",
        "\t# Headline vectorisation\n",
        "\theadline_vectorizer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, stop_words='english')#, max_features=1024)\n",
        "\theadlines_tfidf = headline_vectorizer.fit_transform(training_headlines)\n",
        "\n",
        "\t# Tranform dev/test bodies and headlines using the trained vectorizer (trained on training data)\n",
        "\tbodies_tfidf_dev = body_vectorizer.transform(dev_bodies)\n",
        "\theadlines_tfidf_dev = headline_vectorizer.transform(dev_headlines)\n",
        "\n",
        "\tbodies_tfidf_test = body_vectorizer.transform(test_bodies)\n",
        "\theadlines_tfidf_test = headline_vectorizer.transform(test_headlines)\n",
        "\n",
        "\t# Combine body_tfdif with headline_tfidf for every data point. \n",
        "\ttraining_tfidf = scipy.sparse.hstack([bodies_tfidf, headlines_tfidf])\n",
        "\tdev_tfidf = scipy.sparse.hstack([bodies_tfidf_dev, headlines_tfidf_dev])\n",
        "\ttest_tfidf = scipy.sparse.hstack([bodies_tfidf_test, headlines_tfidf_test])\n",
        "\n",
        "\treturn training_tfidf, dev_tfidf, test_tfidf\n",
        "\n",
        "# Function for extracting the cosine similarity between bodies and headlines. \n",
        "def extract_cosine_similarity(headlines, bodies):\n",
        "\tvectorizer = TfidfVectorizer(ngram_range=(1,2), lowercase=True, stop_words='english')#, max_features=1024)\n",
        "\n",
        "\tcos_sim_features = []\n",
        "\tfor i in range(0, len(bodies)):\n",
        "\t\tbody_vs_headline = []\n",
        "\t\tbody_vs_headline.append(bodies[i])\n",
        "\t\tbody_vs_headline.append(headlines[i])\n",
        "\t\ttfidf = vectorizer.fit_transform(body_vs_headline)\n",
        "\n",
        "\t\tcosine_similarity = (tfidf * tfidf.T).A\n",
        "\t\tcos_sim_features.append(cosine_similarity[0][1])\n",
        "\n",
        "\t# Convert the list to a sparse matrix (in order to concatenate the cos sim with other features)\n",
        "\tcos_sim_array = scipy.sparse.coo_matrix(numpy.array(cos_sim_features)) \n",
        "\n",
        "\treturn cos_sim_array\n",
        "\n",
        "# Function for counting words\n",
        "def extract_word_counts(headlines, bodies):\n",
        "\tword_counts = []\n",
        "\n",
        "\tfor i in range(0, len(headlines)):\n",
        "\t\tfeatures = []\n",
        "\t\tfeatures.append(len(headlines[i].split(\" \")))\n",
        "\t\tfeatures.append(len(bodies[i].split(\" \")))\n",
        "\t\tword_counts.append(features)\n",
        "\tword_counts_array = scipy.sparse.coo_matrix(numpy.array(word_counts))\n",
        "\n",
        "\treturn word_counts_array \n",
        "\n",
        "\n",
        "# Function for combining features of various types (lists, coo_matrix, np.array etc.)\n",
        "def combine_features(tfidf_vectors, cosine_similarity, word_overlap):\n",
        "\tcombined_features =  sparse.bmat([[tfidf_vectors, word_overlap.T, cosine_similarity.T]])\n",
        "\treturn combined_features\n",
        "\n",
        "# Function for extracting features\n",
        "# Feautres: 1) Word Overlap, 2) TF-IDF vectors, 3) Cosine similarity, 4) Word embeddings\n",
        "def extract_features(train, dev, test):\n",
        "\t# Get bodies and headlines for dev and training data\n",
        "\ttraining_bodies = get_bodies(training_data)\n",
        "\ttraining_headlines = get_headlines(training_data)\n",
        "\tdev_bodies = get_bodies(dev_data)\n",
        "\tdev_headlines = get_headlines(dev_data)\n",
        "\ttest_bodies = get_bodies(test_data)\n",
        "\ttest_headlines = get_headlines(test_data)\n",
        "\n",
        "\t# Extract tfidf vectors\n",
        "\tprint(\"\\t-Extracting tfidf vectors..\")\n",
        "\ttraining_tfidf, dev_tfidf, test_tfidf = extract_tfidf(training_headlines, training_bodies, dev_headlines, dev_bodies, test_headlines, test_bodies)\n",
        "\n",
        "\n",
        "\t# Extract word overlap \n",
        "\tprint(\"\\t-Extracting word overlap..\")\n",
        "\ttraining_overlap = extract_word_overlap(training_headlines, training_bodies)\n",
        "\tdev_overlap = extract_word_overlap(dev_headlines, dev_bodies)\n",
        "\ttest_overlap = extract_word_overlap(test_headlines, test_bodies)\n",
        "\n",
        "\t# Extract cosine similarity between bodies and headlines. \n",
        "\tprint(\"\\t-Extracting cosine similarity..\")\n",
        "\ttraining_cos = extract_cosine_similarity(training_headlines, training_bodies)\n",
        "\tdev_cos = extract_cosine_similarity(dev_headlines, dev_bodies)\n",
        "\ttest_cos = extract_cosine_similarity(test_headlines, test_bodies)\n",
        "\n",
        "\t# Combine the features\n",
        "\ttraining_features = combine_features(training_tfidf, training_cos, training_overlap)\n",
        "\tdev_features = combine_features(dev_tfidf, dev_cos, dev_overlap)\n",
        "\ttest_features = combine_features(test_tfidf, test_cos, test_overlap)\n",
        "\n",
        "\treturn training_features, dev_features, test_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEnN8aWQwf0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36b91dc7-a950-4cae-e50e-797bcd6c1b7b"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\t##############################################################################\n",
        "\n",
        "\t# Load the data\n",
        "\tprint(\"\\n[1] Loading data..\")\n",
        "\tdata_splits = split(dataset)\n",
        "    \n",
        "\t# in the format: Stance, Headline, BodyID\n",
        "\ttraining_data = data_splits['training']\n",
        "\tdev_data = data_splits['dev']\n",
        "\ttest_data = data_splits['test'] # currently 0 test points\n",
        "\n",
        "\t# Change the number of training examples used.\n",
        "\tN = int(len(training_data) * 1.0)\n",
        "\ttraining_data = training_data[:N]\n",
        "\n",
        "\tprint(\"\\t-Training size:\\t\", len(training_data))\n",
        "\tprint(\"\\t-Dev size:\\t\", len(dev_data))\n",
        "\tprint(\"\\t-Test data:\\t\", len(test_data))\n",
        "\n",
        "\t##############################################################################\n",
        "\n",
        "\t# Feature extraction\n",
        "\tprint(\"[2] Extracting features.. \")\n",
        "\ttraining_features, dev_features, test_features = extract_features(training_data, dev_data, test_data)\n",
        "\n",
        "\t##############################################################################\n",
        "\n",
        "\t# Fitting model\n",
        "\tprint(\"[3] Fitting model..\")\n",
        "\tprint(\"\\t-Logistic Regression\")\n",
        "\tlr = LogisticRegression(C = 1.0, class_weight='balanced', solver=\"lbfgs\", max_iter=150) \n",
        "    \n",
        "\t#lr = RandomForestClassifier(n_estimators=10, random_state=12345)\n",
        "\t#lr = MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
        "\n",
        "\ttargets_tr = [a['Stance'] for a in training_data]\n",
        "\ttargets_dev = [a['Stance'] for a in dev_data]\n",
        "\ttargets_test = [a['Stance'] for a in test_data]\n",
        "\n",
        "\ty_pred = lr.fit(training_features, targets_tr).predict(test_features)\n",
        "\n",
        "\t##############################################################################\n",
        "\n",
        "\t# Evaluation\n",
        "\tprint(\"[4] Evaluating model..\")\n",
        "\tscore.report_score(targets_test, y_pred)\n",
        "    \n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "85.46593240470791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eK1pnNsxaly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "81106172-3920-4646-8438-221dbfd1ff08"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\t\n",
        "\t# Fitting model\n",
        "\tprint(\"[3] Fitting model..\")\n",
        "\tprint(\"\\t-RandomForestClassifier\")\n",
        "\t\n",
        "    \n",
        "\tlr = RandomForestClassifier(n_estimators=10, random_state=12345)\n",
        "\t#lr = MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
        "\n",
        "\ttargets_tr = [a['Stance'] for a in training_data]\n",
        "\ttargets_dev = [a['Stance'] for a in dev_data]\n",
        "\ttargets_test = [a['Stance'] for a in test_data]\n",
        "\n",
        "\ty_pred = lr.fit(training_features, targets_tr).predict(test_features)\n",
        "\n",
        "\t##############################################################################\n",
        "\n",
        "\t# Evaluation\n",
        "\tprint(\"[4] Evaluating model..\")\n",
        "\tscore.report_score(targets_test, y_pred)\n",
        "    \n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3] Fitting model..\n",
            "\t-RandomForestClassifier\n",
            "[4] Evaluating model..\n",
            "77.38905085843861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lecej1JTxg8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "255bb40f-df17-440a-87b7-2e4edfcf6e1c"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\t\n",
        "\t# Fitting model\n",
        "\tprint(\"[3] Fitting model..\")\n",
        "\tprint(\"\\t-MultinomialNB\")\n",
        "\t\n",
        "    \n",
        "\t\n",
        "\tlr = MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
        "\n",
        "\ttargets_tr = [a['Stance'] for a in training_data]\n",
        "\ttargets_dev = [a['Stance'] for a in dev_data]\n",
        "\ttargets_test = [a['Stance'] for a in test_data]\n",
        "\n",
        "\ty_pred = lr.fit(training_features, targets_tr).predict(test_features)\n",
        "\n",
        "\t##############################################################################\n",
        "\n",
        "\t# Evaluation\n",
        "\tprint(\"[4] Evaluating model..\")\n",
        "\tscore.report_score(targets_test, y_pred)\n",
        "    \n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3] Fitting model..\n",
            "\t-MultinomialNB\n",
            "[4] Evaluating model..\n",
            "55.97667638483965\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}