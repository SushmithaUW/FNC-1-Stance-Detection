{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "LSTM, BI-LSTM with W2V, Glove, Activation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h3nCL7GBPlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip fnc-1.zip\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AIXbyGnH5pN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf_config = tf.ConfigProto()\n",
        "tf_config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=tf_config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFrPwYXPBPlb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d791f56-3064-4273-cb15-229903882633"
      },
      "source": [
        "import gensim\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import matplotlib as mpl\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import plot_model \n",
        "from IPython.display import Image\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(1003)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pFRP93ZBPlg",
        "colab_type": "text"
      },
      "source": [
        "### Python 3.6.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XChgIilrBPlt",
        "colab_type": "text"
      },
      "source": [
        "### Specify Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkcVr6KmBPlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify the folder locations\n",
        "W2V_DIR = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
        "DATA_DIR = 'fnc-1/'\n",
        "Glove = 'glove.6B.200d.txt'\n",
        "\n",
        "# These are some hyperparameters that can be tuned\n",
        "MAX_SENT_LEN = 170\n",
        "MAX_VOCAB_SIZE = 400000\n",
        "LSTM_DIM = 128\n",
        "EMBEDDING_DIM = 200\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 200\n",
        "N_EPOCHS = 10\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cauQcGQ0BPlx",
        "colab_type": "text"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9mFRE-nBPlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the text files of positive and negative sentences\n",
        "train_bodies = pd.read_csv(DATA_DIR+'train_bodies.csv')\n",
        "train_stances = pd.read_csv(DATA_DIR+'train_stances.csv')\n",
        "\n",
        "test_bodies = pd.read_csv(DATA_DIR+'test_bodies.csv')\n",
        "test_stances_unlabeled = pd.read_csv(DATA_DIR+'test_stances_unlabeled.csv')\n",
        "\n",
        "#competetion_bodies = pd.read_csv(DATA_DIR+'competition_test_bodies.csv')\n",
        "#competetion_stances = pd.read_csv(DATA_DIR+'competition_test_stances.csv')\n",
        "\n",
        "#competetion_unlabeled = pd.read_csv(DATA_DIR+'competition_test_stances_unlabeled.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNG5eReVBPl2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d89a1134-908b-41cb-824f-87f5b467dbcb"
      },
      "source": [
        "print('Number of Body sentences:', len(train_bodies['articleBody']))\n",
        "print('Number of Headlines sentences:', len(train_stances['Headline']))\n",
        "\n",
        "train = train_stances.join(train_bodies.set_index('Body ID'), on='Body ID')\n",
        "test = test_stances_unlabeled.join(test_bodies.set_index('Body ID'), on='Body ID')\n",
        "#comp = competetion_stances.join(competetion_bodies.set_index('Body ID'), on='Body ID')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Body sentences: 1683\n",
            "Number of Headlines sentences: 49972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD66lBEfBPl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.replace('unrelated',1,True)\n",
        "train.replace('agree',2,True)\n",
        "train.replace('disagree',3,True)\n",
        "train.replace('discuss',4,True)\n",
        "\n",
        "#comp.replace('unrelated',1,True)\n",
        "#comp.replace('agree',2,True)\n",
        "#comp.replace('disagree',3,True)\n",
        "#comp.replace('discuss',4,True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guPDNT28BPmJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f93ab855-6a19-4d18-a403-99f7e6869d2b"
      },
      "source": [
        "# Pre-processing involves removal of puctuations and converting text to lower case\n",
        "word_seq_headline_train = [text_to_word_sequence(sent) for sent in train['Headline']]\n",
        "word_seq_bodies_train = [text_to_word_sequence(sent) for sent in train['articleBody']]\n",
        "\n",
        "word_seq_headline_test = [text_to_word_sequence(sent) for sent in test['Headline']]\n",
        "word_seq_bodies_test = [text_to_word_sequence(sent) for sent in test['articleBody']]\n",
        "\n",
        "#word_seq_headline_comp = [text_to_word_sequence(sent) for sent in comp['Headline']]\n",
        "#word_seq_bodies_comp = [text_to_word_sequence(sent) for sent in comp['articleBody']]\n",
        "\n",
        "print('90th Percentile Sentence Length:', np.percentile([len(seq) for seq in word_seq_headline_train], 90))\n",
        "print('90th Percentile Sentence Length:', np.percentile([len(seq) for seq in word_seq_bodies_train], 90))\n",
        "\n",
        "print('90th Percentile Sentence Length:', np.percentile([len(seq) for seq in word_seq_headline_test], 90))\n",
        "print('90th Percentile Sentence Length:', np.percentile([len(seq) for seq in word_seq_bodies_test], 90))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90th Percentile Sentence Length: 16.0\n",
            "90th Percentile Sentence Length: 683.0\n",
            "90th Percentile Sentence Length: 16.0\n",
            "90th Percentile Sentence Length: 657.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn2PHD9sXa_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_seq = []\n",
        "for i in range(len(word_seq_headline_train)):\n",
        "  word_seq.append(word_seq_headline_train[i])\n",
        "  \n",
        "for i in range(len(word_seq_bodies_train)):\n",
        "  word_seq.append(word_seq_bodies_train[i])\n",
        "\n",
        "for i in range(len(word_seq_headline_test)):\n",
        "  word_seq.append(word_seq_headline_test[i])\n",
        "\n",
        "for i in range(len(word_seq_bodies_test)):\n",
        "  word_seq.append(word_seq_bodies_test[i])\n",
        "'''  \n",
        "for i in range(len(word_seq_headline_comp)):\n",
        "  word_seq.append(word_seq_headline_comp[i])\n",
        "  \n",
        "for i in range(len(word_seq_bodies_comp)):\n",
        "  word_seq.append(word_seq_bodies_comp[i])\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jsgYwiweMP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range (len(word_seq_headline_train)):\n",
        "  word_seq_headline_train[i].extend(word_seq_bodies_train[i])\n",
        "\n",
        "  \n",
        "for i in range (len(word_seq_headline_test)):\n",
        "  word_seq_headline_test[i].extend(word_seq_bodies_test[i])\n",
        "  \n",
        "\n",
        "#for i in range (len(word_seq_headline_comp)):\n",
        " # word_seq_headline_comp[i].extend(word_seq_bodies_comp[i])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCNFhX55BPmP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4bf4a474-a121-457f-ae1d-3223eaf7f07e"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq])\n",
        "\n",
        "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in vocabulary: 22380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRO2s1RsBPmX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3459c5c9-89e3-43de-dfad-a0c06ed13f39"
      },
      "source": [
        "# Convert the sequence of words to sequnce of indices\n",
        "X_train = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_headline_train])\n",
        "X_train = pad_sequences(X_train, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "y_train = train['Stance']\n",
        "y_train = y_train.values\n",
        "\n",
        "print (X_train.shape)\n",
        "print (y_train.shape)\n",
        "\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_headline_comp])\n",
        "X_test = pad_sequences(X_test, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "\n",
        "#X_comp = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_headline_comp])\n",
        "#X_comp = pad_sequences(X_comp, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "#y_comp = comp['Stance']\n",
        "#y_comp = y_comp.values\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(49972, 170)\n",
            "(49972,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ABqdGoBpNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0a8b2d94-24b6-439a-eabb-5f85929cf629"
      },
      "source": [
        "print (np.unique((y_train)))\n",
        "print (np.unique((y_comp)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4]\n",
            "[1 2 3 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmTwVponVIE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "encoder_train = LabelEncoder()\n",
        "encoder_train.fit(y_train)\n",
        "encoded_train = encoder_train.transform(y_train)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y_train = np_utils.to_categorical(encoded_train)\n",
        "\n",
        "'''\n",
        "encoder_comp = LabelEncoder()\n",
        "encoder_comp.fit(y_comp)\n",
        "encoded_comp = encoder_comp.transform(y_comp)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y_comp = np_utils.to_categorical(encoded_comp)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONYq7augBPmn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cea94f79-720f-4d48-bf57-759924bca8e3"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, dummy_y_train, random_state=10, test_size=0.1)\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=10, test_size=0.1)\n",
        "print (np.unique((y_train)))\n",
        "print (np.unique((y_val)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1.]\n",
            "[0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8UraYlyN0_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = gensim.models.KeyedVectors.load_word2vec_format(W2V_DIR, binary=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIhw7CGQBPm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove2word2vec(glove_input_file=Glove, word2vec_output_file=\"gensim_glove_vectors.txt\")\n",
        "embeddings = gensim.models.KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeRlWLI9BPm6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6724b554-6007-4209-cffd-34f44465e54b"
      },
      "source": [
        "print('Number of words in this pre-trained w2v model:', len(embeddings.vocab))\n",
        "print('Dimension of w2v:', embeddings.vector_size)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in this pre-trained w2v model: 400000\n",
            "Dimension of w2v: 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD-xjcq2BPnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an embedding matrix containing only the word's in our vocabulary\n",
        "# If the word does not have a pre-trained embedding, then randomly initialize the embedding\n",
        "\n",
        "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, EMBEDDING_DIM)) # +1 is because the matrix indices start with 0\n",
        "\n",
        "for word, i in tokenizer.word_index.items(): # i=0 is the embedding for the zero padding\n",
        "    try:\n",
        "        embeddings_vector = embeddings[word]\n",
        "    except KeyError:\n",
        "        embeddings_vector = None\n",
        "    if embeddings_vector is not None:\n",
        "        embeddings_matrix[i] = embeddings_vector\n",
        "        \n",
        "del embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peHyVp5ABPnR",
        "colab_type": "text"
      },
      "source": [
        "### Keras Sequential API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1-RnU3nBPnR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8bb42f5f-3b62-4e59-fe24-a809231343a0"
      },
      "source": [
        "# Build a sequential model by stacking neural net units \n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
        "                          output_dim=EMBEDDING_DIM,\n",
        "                          weights = [embeddings_matrix], trainable=True, name='word_embedding_layer' \n",
        "                          ))\n",
        "model.add(LSTM(LSTM_DIM, return_sequences=False, name='lstm_layer'))\n",
        "#model.add(Bidirectional(LSTM(LSTM_DIM, return_sequences=False, name='Bidrectional_lstm_layer1')))\n",
        "model.add(Dropout(rate=0.8, name='dropout_1')) # Can try varying dropout rates, in paper suggest 0.8\n",
        "model.add(Dense(4, activation='softmax', name='output_layer'))\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0724 19:32:06.858913 140519608772480 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXveDgyABPnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "00d9c07c-9e86-4261-9550-6f32bbe8e88d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "word_embedding_layer (Embedd (None, None, 200)         4476200   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 256)               336896    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 4)                 1028      \n",
            "=================================================================\n",
            "Total params: 4,814,124\n",
            "Trainable params: 4,814,124\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUDZhtpNBPnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkHOvOqHBPnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=N_EPOCHS,\n",
        "          validation_data=(X_val, y_val))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwKFLaLVbBWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('LSTM+W2V', 'wb') as file_pi:\n",
        "        pickle.dump(history.history, file_pi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSZxQTuJhcxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model.save('LSTM+W2V.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "6IgJCLooBPnw",
        "colab_type": "text"
      },
      "source": [
        "**Bi-directional W2V+Glove with activation layer**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIf6S2dEBPnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a sequential model by stacking neural net units \n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
        "                          output_dim=EMBEDDING_DIM,\n",
        "                          weights = [embeddings_matrix], trainable=False, name='word_embedding_layer', \n",
        "                          mask_zero=True)) # trainable=True results in overfitting\n",
        "\n",
        "#model.add(LSTM(LSTM_DIM, return_sequences=False, name='lstm_layer')) # Can try Bidirectional-LSTM\n",
        "model.add(Bidirectional(LSTM(LSTM_DIM, return_sequences=False, name='Bidrectional_lstm_layer1')))\n",
        "\n",
        "model.add(Dense(32, name='dense_1'))\n",
        "# model.add(BatchNormalization(name='bn_1')) # BN did not really help with performance \n",
        "model.add(Dropout(rate=0.3, name='dropout_1')) # Can try varying dropout rates\n",
        "model.add(Activation(activation='relu', name='activation_1'))\n",
        "\n",
        "model.add(Dense(8, name='dense_2'))\n",
        "# model.add(BatchNormalization(name='bn_2'))\n",
        "model.add(Dropout(rate=0.3, name='dropout_2'))\n",
        "model.add(Activation(activation='relu', name='activation_2'))\n",
        "\n",
        "model.add(Dense(4, activation='softmax', name='output_layer'))\n",
        "#model.add(Dense(1, activation='sigmoid', name='output_layer'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BZWnWkbBPn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW14Vp6IBPn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=1,\n",
        "          validation_data=(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgCTRMmeBPn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('Bi-LSTM+Glove+Activation', 'wb') as file_pi:\n",
        "        pickle.dump(history.history, file_pi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFxT2Tg0BPoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model.save('BiLSTMActivation+W2V.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}